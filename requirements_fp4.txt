# Requirements for FP4 VLM Inference on RTX 5090 (Blackwell) via TensorRT-LLM
#
# Models:
#   - nvidia/Cosmos-Reason1-7B (quantized to NVFP4 via quantize_cosmos_fp4.py)
#   - nvidia/Qwen2.5-VL-7B-Instruct-NVFP4 (pre-quantized by NVIDIA)
#
# GPU:    NVIDIA RTX 5090 (Blackwell, SM 10.0+ for native FP4 Tensor Cores)
# Env:    vast.ai Docker instances
#
# Install:
#   pip install -r requirements_fp4.txt --extra-index-url https://pypi.nvidia.com
#
# Note: TensorRT-LLM and its CUDA dependencies are assumed to be pre-installed
#       in the Docker image. This file lists the additional packages needed.
#
# Full setup (including OpenMPI fix, TRT-LLM patches, etc.):
#   bash setup_fp4_vast.sh

# =============================================================================
# Core NVIDIA Packages
# =============================================================================

# TensorRT-LLM for optimized LLM inference with FP4 GEMM kernels
tensorrt-llm>=1.1.0

# TensorRT runtime (usually a dependency of tensorrt-llm)
tensorrt>=10.13.0

# NVIDIA ModelOpt for FP4 quantization (imports as 'modelopt')
nvidia-modelopt>=0.37.0

# CUDA 13 cuBLAS libraries (required by TensorRT-LLM C++ bindings)
nvidia-cublas>=13.0.0

# =============================================================================
# PyTorch
# =============================================================================

torch>=2.9.0

# =============================================================================
# Flash Attention (required by Qwen2.5-VL vision encoder in TRT-LLM)
# =============================================================================

# TRT-LLM sets _attn_implementation='flash_attention_2' for the vision
# transformer. Without this, model loading fails with ImportError.
# Installed separately (needs --no-build-isolation for compilation):
#   pip install flash-attn --no-build-isolation
flash-attn>=2.8.0

# =============================================================================
# Transformers and Model Utilities
# =============================================================================

transformers>=4.56.0
accelerate>=1.7.0
safetensors>=0.4.3
huggingface-hub>=0.23.0

# =============================================================================
# Vision-Language Model (VLM) Utilities
# =============================================================================

# Qwen-VL utilities for video/image frame extraction and preprocessing
qwen-vl-utils>=0.0.14

# OpenCV for video I/O (headless for server/Docker environments)
opencv-python-headless>=4.8.0

# AV for video decoding (used by qwen-vl-utils)
av>=10.0.0

# =============================================================================
# Alternative Quantization Methods
# =============================================================================

# bitsandbytes for INT8/INT4 quantization (used by fp8_inference.py)
bitsandbytes>=0.45.0

# =============================================================================
# 3D Point Cloud (STU Dataset)
# =============================================================================

# Open3D for point cloud processing (used by stu_dataset experiments)
open3d>=0.18.0

# =============================================================================
# Calibration & Datasets
# =============================================================================

# HuggingFace datasets for calibration data (CNN/DailyMail used by quantize_cosmos_fp4.py)
datasets>=2.14.0

# =============================================================================
# Other Dependencies
# =============================================================================

# NumPy <2 required for TensorRT-LLM compatibility
numpy<2

pydantic>=2.9.0
rich>=13.0.0
pillow>=10.0.0
torchvision>=0.20.0
